{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e5253ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from citation_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6f927d",
   "metadata": {},
   "source": [
    "## Spacy docs\n",
    "https://spacy.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a55139de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U spacy\n",
    "# python -m spacy download en_core_web_sm\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a15d58b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English tokenizer, tagger, parser and NER\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# https://github.com/explosion/spaCy/issues/4577"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6dee20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process whole documents\n",
    "text = (\"When Sebastian Thrun started working on self-driving cars at \"\n",
    "        \"Google in 2007, few people outside of the company took him \"\n",
    "        \"seriously. “I can tell you very senior CEOs of major American \"\n",
    "        \"car companies would shake my hand and turn away because I wasn’t \"\n",
    "        \"worth talking to,” said Thrun, in an interview with Recode earlier \"\n",
    "        \"this week.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77aef2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "766c9700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun phrases: ['Sebastian Thrun', 'self-driving cars', 'Google', 'few people', 'the company', 'him', 'I', 'you', 'very senior CEOs', 'major American car companies', 'my hand', 'I', 'Thrun', 'an interview', 'Recode']\n"
     ]
    }
   ],
   "source": [
    "# Analyze syntax\n",
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bd600b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verbs: ['start', 'work', 'drive', 'take', 'tell', 'shake', 'turn', 'talk', 'say']\n"
     ]
    }
   ],
   "source": [
    "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1820e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sebastian Thrun PERSON\n",
      "Google ORG\n",
      "2007 DATE\n",
      "American NORP\n",
      "Thrun PERSON\n",
      "Recode ORG\n",
      "earlier this week DATE\n"
     ]
    }
   ],
   "source": [
    "# Find named entities, phrases and concepts\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be237f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: perform analysis on a paragraph from a word doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e1fc3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vanilla_fixer = CitationFixer()\n",
    "# vanilla_fixer.read_document(\"AA final paper copy.docx\")\n",
    "# vanilla_fixer.get_citations()\n",
    "# vanilla_fixer.fix_citations()\n",
    "# vanilla_fixer.save_document(\"AA final paper copy_fixed.docx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59963d3b",
   "metadata": {},
   "source": [
    "## Using NLP to analyze paragraphs from the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13e49eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_fixer = CitationFixer()\n",
    "vanilla_fixer.read_document(\"AA final paper copy.docx\")\n",
    "vanilla_fixer.get_citations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90fa3652",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = vanilla_fixer.document.paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72b4d192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91a59021",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12e770d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“The Endless Frontiers [sic] Act is a downpayment for future generations of American technological leadership, and I'm proud to introduce it on a bipartisan basis.” – Representative Mike Gallagher (2020).\n",
      "\n",
      "“The point is that founding and growing a company is fundamentally an act of exploration and colonization… Google took web search… Twitter colonized real-time status updates. Quora is attempting to colonize Q&A… Facebook of course colonized online identity.” – Kevin Simler (2012).\n"
     ]
    }
   ],
   "source": [
    "paragraph = paragraphs[7].text\n",
    "print(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51c3c662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Endless Frontiers 1 22 WORK_OF_ART\n",
      "American 76 84 NORP\n",
      "Mike Gallagher 182 196 PERSON\n",
      "2020 198 202 DATE\n",
      "Google 313 319 ORG\n",
      "Twitter 337 344 PERSON\n",
      "Quora 381 386 PERSON\n",
      "Q&A 413 416 ORG\n",
      "Kevin Simler 467 479 PERSON\n",
      "2012 481 485 DATE\n"
     ]
    }
   ],
   "source": [
    "# named entity recognition\n",
    "doc = nlp(paragraph)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "    \n",
    "# observation: a citation often occurs with PERSON followed by DATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d85481d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(doc.ents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9854ef8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# print(type(doc.ents[0].label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085e641c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51b94330",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNLPCitationFixer(CitationFixer):\n",
    "    def __init__(self):\n",
    "        self.min_paragraph_length = 10\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "    def get_citations(self):\n",
    "        print(\"inside get_citations nlp\")\n",
    "        print(\"len[self.citations]:\", len(self.citations))\n",
    "        for i in range(len(self.document.paragraphs)):\n",
    "            text = self.document.paragraphs[i].text\n",
    "            if len(text) >= self.min_paragraph_length:\n",
    "                entities = self.nlp(text).ents\n",
    "                for j in range(len(entities)-1):\n",
    "                    if entities[j].label_ == 'PERSON' and entities[j+1].label_ == 'DATE':\n",
    "                        start_char = entities[j].start_char\n",
    "                        end_char = entities[j+1].end_char\n",
    "                        # spacy gives the intervals in the [start_char, end_char) format,\n",
    "                        # but we want it to be in the [start_char, end_char] format.\n",
    "                        # Since we want to include ')', we don't decrement end_char\n",
    "                        if text[end_char-len(entities[j+1].text)-1] == '(':\n",
    "                            # ensuring that there is a ( before the year (distinguishing it from cases\n",
    "                            # there is a year but not for the purpose of a citation)\n",
    "                            self.citations[i].append((start_char, end_char))\n",
    "#                             print(\"last character:\")\n",
    "#                             print(\"citation:\", text[start_char:end_char+1])\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d576bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading document\n",
      "inside get_citations nlp\n",
      "len[self.citations]: 54\n",
      "citation: Mike Gallagher (2020)\n",
      "citation: Kevin Simler (2012)\n"
     ]
    }
   ],
   "source": [
    "nlp_fixer = SimpleNLPCitationFixer()\n",
    "nlp_fixer.read_document(\"AA final paper copy.docx\")\n",
    "nlp_fixer.get_citations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af14557",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341c4569",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
